{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Defining the Study Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from osgeo import gdal\n",
    "import ee\n",
    "\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "import rioxarray as rxr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load config.yml\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Get project root (adjust based on your folder depth)\n",
    "current_dir = Path(os.getcwd())\n",
    "project_root = current_dir.parent.parent  # Navigate up from \"Scripts/Phase1_Data_Preprocessing\"\n",
    "\n",
    "with open(project_root / \"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Print the config dictionary to debug\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Construct paths\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Raw data paths\n",
    "raw_data_dir = project_root / config[\"paths\"][\"raw_data\"]\n",
    "soil_raw_dir = raw_data_dir / \"GIS/Soil\"  # Matches your hardcoded path structure\n",
    "morocco_path = raw_data_dir / config[\"paths\"][\"morocco_path\"]\n",
    "tadla_plain_path = raw_data_dir / config[\"paths\"][\"tadla_plain_raw\"]\n",
    "tadla_plain_boundary_path = raw_data_dir / config[\"paths\"][\"tadla_plain_boundary_raw\"]\n",
    "soil_raw_path = raw_data_dir / config[\"paths\"][\"soil_raw\"]\n",
    "dem_raw_path = raw_data_dir / config[\"paths\"][\"dem_raw\"]\n",
    "chirps_raw_path = raw_data_dir / config[\"paths\"][\"chirps_raw\"]\n",
    "era5_raw_path = raw_data_dir / config[\"paths\"][\"era5_raw\"]\n",
    "wv0010_raw_path = raw_data_dir / config[\"paths\"][\"wv0010_raw\"]\n",
    "ndvi_path = raw_data_dir / config[\"paths\"][\"ndvi_raw\"]\n",
    "\n",
    "\n",
    "land_use_raw_dir = raw_data_dir / config[\"paths\"][\"land_use_raw\"]\n",
    "\n",
    "\n",
    "# Processed data paths\n",
    "processed_data_dir = project_root / config[\"paths\"][\"processed_data\"]\n",
    "soil_processed_dir = processed_data_dir / \"GIS/Soil\"\n",
    "output_dir = processed_data_dir / \"GIS/Study_Area_Boundary\"\n",
    "output_path = output_dir / \"Tadla_plain_common.shp\"\n",
    "tadla_common_path = processed_data_dir / config[\"paths\"][\"tadla_boundary_processed\"]\n",
    "soil_processed_path = processed_data_dir / config[\"paths\"][\"soil_processed\"]\n",
    "dem_processed_path = processed_data_dir / config[\"paths\"][\"dem_processed\"]\n",
    "slope_path = processed_data_dir / \"GIS/Topography/tadla_slope.tif\"\n",
    "aspect_path = processed_data_dir / \"GIS/Topography/tadla_aspect.tif\"\n",
    "chirps_processed_path = processed_data_dir / config[\"paths\"][\"chirps_processed\"]\n",
    "era5_processed_path = processed_data_dir / config[\"paths\"][\"era5_processed\"]\n",
    "wv0010_processed_path = processed_data_dir / config[\"paths\"][\"wv0010_processed\"]\n",
    "topography_processed_dir = processed_data_dir / \"GIS/Topography\"\n",
    "\n",
    "land_use_processed_dir = processed_data_dir / config[\"paths\"][\"land_use_processed\"]\n",
    "\n",
    "# Harmonized data paths\n",
    "harmonized_dir = Path(config[\"paths\"][\"harmonized_data\"])\n",
    "weather_processed_dir = processed_data_dir / \"Weather\"\n",
    "chirps_output_dir = Path(config[\"paths\"][\"chirps_dir\"])\n",
    "\n",
    "output_path_dataset = harmonized_dir / \"tadla_spatiotemporal_dataset.nc\"\n",
    "\n",
    "\n",
    "\n",
    "# Ensure output directories exist\n",
    "harmonized_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(era5_processed_path.parent, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Morocco boundary\n",
    "morocco = gpd.read_file(morocco_path)\n",
    "\n",
    "# Check the first few rows to see province names\n",
    "morocco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(morocco.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morocco_merchiche = morocco.to_crs(epsg=26191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morocco_merchiche.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tadla Plain shapefile\n",
    "tadla_plain_polygon = gpd.read_file(tadla_plain_path)\n",
    "\n",
    "# Check the data\n",
    "print(tadla_plain_polygon)  # Show first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadla_plain_polygon.plot()  # Plot the geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Study area size: {tadla_plain_polygon.geometry.area} m²\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject to Merchich (EPSG:26191)\n",
    "tadla_merchiche = tadla_plain_polygon.to_crs(epsg=26191)\n",
    "\n",
    "# Calculate area\n",
    "area_m2 = tadla_merchiche.geometry.area\n",
    "print(f\"Study area size: {area_m2[0]:.2f} m²\")  \n",
    "# Example output: \"Study area size: 1300000000.00 m²\"\n",
    "\n",
    "area_ha = area_m2 / 10000\n",
    "print(f\"Study area size: {area_ha[0]:.2f} hectares\")  \n",
    "# Example output: \"Study area size: 130000.00 hectares\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadla_merchiche.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned boundary shapefile\n",
    "Tadla_plain_boundary = gpd.read_file(tadla_plain_boundary_path)\n",
    "# Check the current CRS\n",
    "print(Tadla_plain_boundary.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Merchich CRS if needed\n",
    "if Tadla_plain_boundary.crs != \"EPSG:26191\":\n",
    "    Tadla_plain_boundary = Tadla_plain_boundary.to_crs(epsg=26191)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tadla_plain_boundary.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume these are already loaded and in the same CRS (EPSG:26191)\n",
    "# tadla_merchiche: full administrative boundary (Merchich)\n",
    "# tadla_plain_polygone: digitized Tadla plain (which may be slightly off)\n",
    "\n",
    "# Compute the common (intersecting) area between the two layers\n",
    "tadla_plain = gpd.overlay(Tadla_plain_boundary, tadla_merchiche, how='intersection')\n",
    "\n",
    "# Save the resulting common area shapefile for further analysis\n",
    "tadla_plain.to_file(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot layers with explicit labels\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "tadla_merchiche.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\", linewidth=2)\n",
    "Tadla_plain_boundary.plot(ax=ax, facecolor=\"blue\", alpha=0.5, edgecolor=\"blue\")\n",
    "tadla_plain.plot(ax=ax, facecolor=\"green\", alpha=0.5, edgecolor=\"black\")\n",
    "\n",
    "# Create custom legend\n",
    "legend_labels = {\n",
    "    \"Full Admin Boundary\": \"red\",\n",
    "    \"Digitized Tadla Plain\": \"blue\",\n",
    "    \"Common Area\": \"green\"\n",
    "}\n",
    "patches = [Patch(color=color, label=label) for label, color in legend_labels.items()]\n",
    "plt.legend(handles=patches)\n",
    "\n",
    "plt.title(\"Common Area between Tadla Plain and Full Admin Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot layers with explicit labels\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "tadla_merchiche.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\", linewidth=2)\n",
    "Tadla_plain_boundary.plot(ax=ax, facecolor=\"blue\", alpha=0.5, edgecolor=\"blue\")\n",
    "tadla_plain.plot(ax=ax, facecolor=\"green\", alpha=0.5, edgecolor=\"black\")\n",
    "morocco_merchiche.plot(ax=ax, facecolor=\"none\", edgecolor=\"brown\", linewidth=1)\n",
    "\n",
    "# Create custom legend\n",
    "legend_labels = {\n",
    "    \"Full Admin Boundary\": \"red\",\n",
    "    \"Digitized Tadla Plain\": \"blue\",\n",
    "    \"Common Area\": \"green\",\n",
    "    \"Morocco\": \"brown\"\n",
    "}\n",
    "patches = [Patch(color=color, label=label) for label, color in legend_labels.items()]\n",
    "plt.legend(handles=patches)\n",
    "\n",
    "plt.title(\"Common Area between Tadla Plain and Full Admin Boundary of Morocco\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadla_plain = tadla_plain.to_crs(epsg=26191)  # Ensure projection\n",
    "tadla_merchiche = tadla_merchiche.to_crs(epsg=26191)\n",
    "\n",
    "area_plain_m2 = tadla_plain.geometry.area.sum()\n",
    "area_full_m2 = tadla_merchiche.geometry.area.sum()\n",
    "\n",
    "print(f\"Tadla Plain area: {area_plain_m2:.2f} m²\")\n",
    "print(f\"Full Admin Boundary area: {area_full_m2:.2f} m²\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def reproject_raster(input_path, output_path, target_crs):\n",
    "    with rasterio.open(input_path) as src:\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, target_crs, src.width, src.height, *src.bounds\n",
    "        )\n",
    "        metadata = src.meta.copy()\n",
    "        metadata.update({\n",
    "            \"crs\": target_crs,\n",
    "            \"transform\": transform,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "        with rasterio.open(output_path, \"w\", **metadata) as dest:\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=rasterio.band(dest, 1),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=target_crs\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Downloading Soil Data (SoilGrids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Tadla boundary (EPSG:26191)\n",
    "tadla = gpd.read_file(tadla_common_path)\n",
    "tadla = tadla.to_crs(\"EPSG:26191\")\n",
    "\n",
    "# Get bounding box in Merchich coordinates\n",
    "minx, miny, maxx, maxy = tadla.total_bounds\n",
    "print(f\"X: {minx}, {maxx}\")  # Easting bounds\n",
    "print(f\"Y: {miny}, {maxy}\")  # Northing bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box of Tadla Plain in EPSG:26191 (from your URL)\n",
    "minx, maxx = 301450, 490490  # X (Easting)\n",
    "miny, maxy = 158150, 244870   # Y (Northing)\n",
    "\n",
    "# Soil layers and their COVERAGEIDs (adjust if needed)\n",
    "layers = {\n",
    "    \"clay\": \"clay_0-5cm_mean\",\n",
    "    \"silt\": \"silt_0-5cm_mean\",\n",
    "    \"sand\": \"sand_0-5cm_mean\",\n",
    "    \"ocd\": \"ocd_0-5cm_mean\",    # Organic carbon density\n",
    "    \"wv0010\": \"wv0010_0-5cm_mean\"     # Water content at saturation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "\n",
    "# Create a transformer from EPSG:4326 to EPSG:26191\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:26191\", always_xy=True)\n",
    "\n",
    "# Transform the lower-left corner (-7.5, 32.0)\n",
    "easting_min, northing_min = transformer.transform(-7.5, 32.0)\n",
    "print(easting_min, northing_min)  # Expected: ~339200, ~164400\n",
    "\n",
    "# Transform the upper-right corner (-5.5, 32.8)\n",
    "easting_max, northing_max = transformer.transform(-5.5, 32.8)\n",
    "print(easting_max, northing_max)  # Expected: ~459750, ~241200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Python Script to Download All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(soil_raw_dir, exist_ok=True)\n",
    "\n",
    "for param, coverage_id in layers.items():\n",
    "    url = (\n",
    "        f\"https://maps.isric.org/mapserv?map=/map/{param}.map&\"\n",
    "        f\"SERVICE=WCS&\"\n",
    "        f\"VERSION=2.0.1&\"\n",
    "        f\"REQUEST=GetCoverage&\"\n",
    "        f\"COVERAGEID={coverage_id}&\"\n",
    "        f\"FORMAT=GEOTIFF_INT16&\"  # Or GEOTIFF_FLOAT32 for raw values\n",
    "        f\"SUBSET=X({minx},{maxx})&\"\n",
    "        f\"SUBSET=Y({miny},{maxy})&\"\n",
    "        f\"SUBSETTINGCRS=http://www.opengis.net/def/crs/EPSG/0/26191&\"\n",
    "        f\"OUTPUTCRS=http://www.opengis.net/def/crs/EPSG/0/26191\"\n",
    "    )\n",
    "    print(url)\n",
    "    \n",
    "    # Download and save\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        output_path = os.path.join(soil_raw_dir, f\"tadla_{param}.tif\")\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {param} to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {param}: HTTP {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Unit Conversion:\n",
    "\n",
    "    SoilGrids stores integer values as actual value × 10. \n",
    "    \n",
    "    For example:\n",
    "        A pixel value of 150 = 15% clay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process soil data\n",
    "\n",
    "    # = src.profile\n",
    "   \n",
    "\n",
    "with rasterio.open(soil_raw_path) as src:\n",
    "    clay = src.read(1)\n",
    "    clay = clay.astype(np.float32) / 10  # Convert to %\n",
    "    profile = src.profile.copy()\n",
    "    profile.update(dtype=rasterio.float32)\n",
    "\n",
    "    with rasterio.open(soil_processed_path, \"w\", **profile) as dst:\n",
    "        dst.write(src.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(soil_processed_path) as src:\n",
    "    print(src.res)  # Should output (10.0, 10.0)\n",
    "    print(src.read(1).min(), src.read(1).max())  # e.g., 0.0–38.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(ndvi_path) as ndvi_src:\n",
    "    ndvi_data = ndvi_src.read(1)\n",
    "    ndvi_transform = ndvi_src.transform\n",
    "    ndvi_crs = ndvi_src.crs\n",
    "    ndvi_width = ndvi_src.width\n",
    "    ndvi_height = ndvi_src.height\n",
    "\n",
    "with rasterio.open(soil_processed_path) as soil_src:\n",
    "    soil_data = soil_src.read(1)\n",
    "    soil_crs = soil_src.crs\n",
    "    soil_transform = soil_src.transform\n",
    "\n",
    "# Create an empty array matching NDVI's shape\n",
    "soil_reproj = np.empty((ndvi_height, ndvi_width), dtype=soil_data.dtype)\n",
    "\n",
    "# Force the clay raster onto NDVI's exact grid\n",
    "reproject(\n",
    "    source=soil_data,\n",
    "    destination=soil_reproj,\n",
    "    src_transform=soil_transform,\n",
    "    src_crs=soil_crs,\n",
    "    dst_transform=ndvi_transform,\n",
    "    dst_crs=ndvi_crs,\n",
    "    resampling=Resampling.nearest\n",
    ")\n",
    "\n",
    "# Now ndvi_data and soil_reproj have the same shape and alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_area_path = output_dir / \"Tadla_plain_common.shp\"\n",
    "study_area = gpd.read_file(study_area_path).to_crs(ndvi_crs)\n",
    "\n",
    "# 1. Open and plot the clay raster\n",
    "with rasterio.open(soil_processed_path) as src:\n",
    "    clay_crs = src.crs\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    show(src, ax=ax, cmap=\"Reds\", alpha=0.7)\n",
    "    ax.set_title(\"Clay Map with Study Area Boundary\")\n",
    "\n",
    "# 2. Load and reproject the study area to the clay's CRS\n",
    "study_area = gpd.read_file(study_area_path)\n",
    "study_area = study_area.to_crs(clay_crs)\n",
    "\n",
    "# 3. Overlay the boundary on the same axes\n",
    "study_area.plot(\n",
    "    ax=ax,\n",
    "    facecolor=\"none\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open raw water content data\n",
    "with rasterio.open(wv0010_raw_path) as src:\n",
    "    data = src.read(1) / 10  # Convert to %\n",
    "    profile = src.profile.copy()\n",
    "\n",
    "    # Calculate new dimensions for 10m resolution\n",
    "    new_width = int(src.width * (src.res[0] / 10))  # From ~326m → 10m\n",
    "    new_height = int(src.height * (abs(src.res[1]) / 10))  # From ~533m → 10m\n",
    "\n",
    "    # Create empty array for resampled data\n",
    "    resampled_data = np.empty((new_height, new_width), dtype=np.float32)\n",
    "\n",
    "    # Define target transform for 10m resolution\n",
    "    target_transform = rasterio.Affine(10, 0, src.bounds.left, 0, -10, src.bounds.top)\n",
    "\n",
    "    # Resample using bilinear interpolation\n",
    "    reproject(\n",
    "        source=data,\n",
    "        destination=resampled_data,\n",
    "        src_transform=src.transform,\n",
    "        dst_transform=target_transform,\n",
    "        src_crs=src.crs,\n",
    "        dst_crs=src.crs,\n",
    "        resampling=Resampling.bilinear\n",
    "    )\n",
    "\n",
    "# Update metadata for the processed file\n",
    "profile.update({\n",
    "    \"transform\": target_transform,\n",
    "    \"width\": new_width,\n",
    "    \"height\": new_height,\n",
    "    \"dtype\": \"float32\"\n",
    "})\n",
    "\n",
    "# Save resampled data\n",
    "with rasterio.open(wv0010_processed_path, \"w\", **profile) as dst:\n",
    "    dst.write(resampled_data, 1)\n",
    "\n",
    "print(f\"Resampled water content saved to: {wv0010_processed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(wv0010_processed_path) as src:\n",
    "    print(src.res)  # Should output (10.0, 10.0)\n",
    "    print(src.read(1).min(), src.read(1).max())  # e.g., 0.0–38.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Validate CRS Alignment\n",
    "\n",
    "    Confirm all downloaded rasters are in EPSG:26191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with rasterio.open(soil_raw_path) as src:\n",
    "    print(src.crs)  # Should print \"EPSG:26191\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: DEM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download DEM Data\n",
    "\n",
    "    We’ll use ALOS PALSAR Global DEM (12.5m resolution) from Google Earth Engine (GEE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Tadla boundary (ensure this path is correct)\n",
    "tadla_shp_path = tadla_common_path\n",
    "tadla = gpd.read_file(tadla_shp_path)\n",
    "\n",
    "# Check current CRS\n",
    "print(f\"Current CRS: {tadla.crs}\")  # Should be EPSG:26191 (Merchich)\n",
    "\n",
    "# Reproject to WGS84 (EPSG:4326)\n",
    "tadla_wgs84 = tadla.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Save reprojected shapefile\n",
    "tadla_wgs84.to_file(tadla_shp_path)  # Overwrite or save to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "project_id = os.environ.get('GCP_PROJECT')\n",
    "if not project_id:\n",
    "    raise ValueError(\"The environment variable GCP_PROJECT is not set.\")\n",
    "\n",
    "print(\"Using project ID:\", project_id)\n",
    "\n",
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test authentication\n",
    "print(ee.Image(\"NASA/NASADEM_HGT/001\").get(\"title\").getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = ee.Geometry.Rectangle(\n",
    "    [-7.5, 32.0, -5.5, 32.8],  # minx, miny, maxx, maxy\n",
    "    proj=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ALOS DEM ImageCollection and select the 'DSM' band\n",
    "dem_collection = ee.ImageCollection(\"JAXA/ALOS/AW3D30/V3_2\").select('DSM')\n",
    "\n",
    "# Mosaic the collection into a single image (combines all tiles over Tadla)\n",
    "dem = dem_collection.mosaic().clip(bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Google Drive\n",
    "task = ee.batch.Export.image.toDrive(\n",
    "    image=dem,\n",
    "    description='Tadla_DEM',\n",
    "    folder='Tadla_Project',\n",
    "    scale=12.5,\n",
    "    region=bbox,\n",
    "    crs=\"EPSG:26191\",  # Merchich CRS\n",
    "    fileFormat='GeoTIFF',\n",
    "    maxPixels=1e13\n",
    ")\n",
    "task.start()\n",
    "\n",
    "# Monitor task progress\n",
    "print(f\"Task ID: {task.id}\")\n",
    "print(\"Check progress at: https://code.earthengine.google.com/tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocess DEM\n",
    "    \n",
    "    Once downloaded, move the DEM to Data/Raw/GIS/Topography/ and preprocess it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load boundary and ensure it's in the same CRS as the DEM (EPSG:26191)\n",
    "tadla = gpd.read_file(tadla_common_path)\n",
    "if tadla.crs != \"EPSG:26191\":\n",
    "    tadla = tadla.to_crs(\"EPSG:26191\")\n",
    "\n",
    "# Load DEM and check its CRS\n",
    "with rasterio.open(dem_raw_path) as src:\n",
    "    dem_crs = src.crs\n",
    "    print(f\"DEM CRS: {dem_crs}\")  # Should be EPSG:26191\n",
    "\n",
    "    # Fix 2: Reproject boundary if DEM is in a different CRS\n",
    "    if tadla.crs != dem_crs:\n",
    "        tadla = tadla.to_crs(dem_crs)\n",
    "\n",
    "    # Fix 3: Validate overlap\n",
    "    dem_bounds = src.bounds\n",
    "    tadla_bounds = tadla.total_bounds\n",
    "    print(f\"DEM Bounds: {dem_bounds}\")\n",
    "    print(f\"Tadla Bounds: {tadla_bounds}\")\n",
    "\n",
    "    if not (\n",
    "        (tadla_bounds[0] > dem_bounds.left) &\n",
    "        (tadla_bounds[2] < dem_bounds.right) &\n",
    "        (tadla_bounds[1] > dem_bounds.bottom) &\n",
    "        (tadla_bounds[3] < dem_bounds.top)\n",
    "    ):\n",
    "        raise ValueError(\"DEM and boundary do not overlap. Check their geographic extents!\")\n",
    "\n",
    "    # Clip DEM\n",
    "    tadla_dem, transform = mask(src, tadla.geometry, crop=True)\n",
    "    meta = src.meta.copy()\n",
    "    meta.update({\n",
    "        \"height\": tadla_dem.shape[1],\n",
    "        \"width\": tadla_dem.shape[2],\n",
    "        \"transform\": transform,\n",
    "        \"crs\": dem_crs\n",
    "    })\n",
    "\n",
    "# Save clipped DEM\n",
    "with rasterio.open(dem_processed_path, \"w\", **meta) as dest:\n",
    "    dest.write(tadla_dem)\n",
    "print(f\"Clipped DEM saved to: {dem_processed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DEM exists: {dem_raw_path.exists()}\")\n",
    "print(f\"Boundary exists: {tadla_common_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Derive Slope and Aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculating Slope and Aspect Using GDAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GDAL exceptions\n",
    "gdal.UseExceptions()\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(slope_path.parent, exist_ok=True)\n",
    "\n",
    "# Calculate slope\n",
    "slope = gdal.DEMProcessing(\n",
    "    destName=str(slope_path),\n",
    "    srcDS=str(dem_processed_path),\n",
    "    processing=\"slope\",\n",
    "    format=\"GTiff\",\n",
    "    slopeFormat=\"degree\"\n",
    ")\n",
    "\n",
    "# Calculate aspect\n",
    "aspect = gdal.DEMProcessing(\n",
    "    destName=str(aspect_path),\n",
    "    srcDS=str(dem_processed_path),\n",
    "    processing=\"aspect\",\n",
    "    format=\"GTiff\"\n",
    ")\n",
    "\n",
    "print(f\"Slope saved to: {slope_path}\")\n",
    "print(f\"Aspect saved to: {aspect_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download CHIRPS Rainfall Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate and initialize GEE\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project=project_id)\n",
    "\n",
    "# Define study area (Tadla Plain)\n",
    "tadla_plain = ee.Geometry.Rectangle(\n",
    "    [-7.5, 32.0, -5.5, 32.8],  # Adjust to your exact boundary\n",
    "    proj=\"EPSG:4326\",\n",
    "    geodesic=False\n",
    ")\n",
    "\n",
    "# Load CHIRPS daily rainfall (2017–2023)\n",
    "chirps = ee.ImageCollection(\"UCSB-CHG/CHIRPS/DAILY\") \\\n",
    "    .filterDate(\"2017-01-01\", \"2023-12-31\") \\\n",
    "    .filterBounds(tadla_plain)\n",
    "\n",
    "# Convert to multi-band image (one band per day)\n",
    "chirps_multi_band = chirps.toBands()\n",
    "\n",
    "# Export to Google Drive\n",
    "task = ee.batch.Export.image.toDrive(\n",
    "    image=chirps_multi_band,\n",
    "    description=\"CHIRPS_Daily_Tadla_2017-2023\",\n",
    "    folder=\"Tadla_Project\",\n",
    "    fileNamePrefix=\"CHIRPS_Daily_Tadla_2017-2023\",\n",
    "    region=tadla_plain,\n",
    "    scale=5000,  # CHIRPS native resolution (~5km)\n",
    "    crs=\"EPSG:4326\",  # Reproject to EPSG:26191 later in Python\n",
    "    maxPixels=1e13,\n",
    "    fileFormat=\"GeoTIFF\"\n",
    ")\n",
    "\n",
    "task.start()\n",
    "print(\"Export started! Monitor at: https://code.earthengine.google.com/tasks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Download ERA5 Temperature/ET Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ee.Initialize(project=project_id)\n",
    "# Define study area\n",
    "tadla_plain = ee.Geometry.Rectangle(\n",
    "    [-7.5, 32.0, -5.5, 32.8], \n",
    "    proj=\"EPSG:4326\",\n",
    "    geodesic=False\n",
    ")\n",
    "\n",
    "# Load ERA5-Land DAILY_AGGR and select evaporation\n",
    "era5_land = ee.ImageCollection(\"ECMWF/ERA5_LAND/DAILY_AGGR\") \\\n",
    "    .filterDate(\"2017-01-01\", \"2023-12-31\") \\\n",
    "    .filterBounds(tadla_plain) \\\n",
    "    .select(\"total_evaporation_sum\")  # <--- CORRECT BAND NAME\n",
    "\n",
    "# Convert to multi-band image (one band per day)\n",
    "era5_multi_band = era5_land.toBands()\n",
    "\n",
    "# Export to Google Drive\n",
    "task = ee.batch.Export.image.toDrive(\n",
    "    image=era5_multi_band,\n",
    "    description=\"ERA5_Land_Evaporation_Tadla_2017-2023\",\n",
    "    folder=\"Tadla_Project\",\n",
    "    fileNamePrefix=\"ERA5_Land_Evaporation_Tadla_2017-2023\",\n",
    "    region=tadla_plain,\n",
    "    scale=11132,  # ERA5-Land resolution: 0.1° (~11km)\n",
    "    crs=\"EPSG:4326\",\n",
    "    maxPixels=1e13,\n",
    "    fileFormat=\"GeoTIFF\"\n",
    ")\n",
    "\n",
    "task.start()\n",
    "print(\"Export started! Monitor progress at: https://code.earthengine.google.com/tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Preprocess CHIRPS Rainfall Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CHIRPS data with simplified band names\n",
    "chirps_ds = rxr.open_rasterio(chirps_raw_path)\n",
    "\n",
    "# Generate dates for 2017-01-01 to 2023-12-31\n",
    "dates = pd.date_range(start=\"2017-01-01\", periods=chirps_ds.sizes[\"band\"], freq=\"D\")\n",
    "\n",
    "# Assign time coordinates to the 'band' dimension\n",
    "chirps_ds = chirps_ds.assign_coords(band=dates).rename({\"band\": \"time\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Monthly Aggregated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "def process_month(year, month, spatial_chunk=128):\n",
    "    \"\"\"Aggregate daily data for one month, reproject, clip, and save as GeoTIFF.\"\"\"\n",
    "    try:\n",
    "        # Adjust worker settings based on your system.\n",
    "        cluster = LocalCluster(n_workers=4, memory_limit='8GB')\n",
    "        client = Client(cluster)\n",
    "        \n",
    "        # Determine start and end dates for the month\n",
    "        start_date = f\"{year}-{month:02d}-01\"\n",
    "        end_day = pd.Timestamp(year, month, 1).daysinmonth\n",
    "        end_date = f\"{year}-{month:02d}-{end_day:02d}\"\n",
    "        \n",
    "        # Select daily data for the month and aggregate (sum) over time\n",
    "        ds_month = chirps_ds.sel(time=slice(start_date, end_date))\n",
    "        ds_month_agg = ds_month.sum(dim=\"time\")\n",
    "        \n",
    "        # Reproject to target CRS (EPSG:26191) with 10 m resolution\n",
    "        ds_reproj = ds_month_agg.rio.reproject(\n",
    "            \"EPSG:26191\",\n",
    "            resolution=10,\n",
    "            resampling=Resampling.bilinear,\n",
    "            nodata=np.nan\n",
    "        )\n",
    "        \n",
    "        # Read Tadla boundary from your shapefile or other source\n",
    "        tadla = gpd.read_file(tadla_common_path)\n",
    "        \n",
    "        # Clip to the Tadla boundary\n",
    "        ds_clipped = ds_reproj.rio.clip(tadla.geometry, tadla.crs)\n",
    "        \n",
    "        # Remove problematic attributes if present (e.g., \"long_name\")\n",
    "        ds_clipped.attrs.pop(\"long_name\", None)\n",
    "        # If needed, also remove it from specific data variables, e.g.,\n",
    "        # ds_clipped['precipitation'].attrs.pop(\"long_name\", None)\n",
    "        \n",
    "        # Save the monthly aggregated data to a GeoTIFF file\n",
    "        out_filename = weather_processed_dir / f\"CHIRPS_{year}_{month:02d}.tif\"\n",
    "        ds_clipped.rio.to_raster(out_filename)\n",
    "        print(f\"Saved {out_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {year}-{month:02d}: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each month in your desired time period (2017-2023)\n",
    "for year in range(2017, 2024):\n",
    "    for month in range(1, 13):\n",
    "        process_month(year, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Preprocess ERA5 Evaporation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Process ERA5 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "\n",
    "# Load ERA5 data (replace with your path)\n",
    "era5_ds = rxr.open_rasterio(era5_raw_path, chunks={\"band\": 1, \"x\": 256, \"y\": 256})\n",
    "\n",
    "# Assign time coordinates (assuming bands are daily from 2017-01-01)\n",
    "dates = pd.date_range(start=\"2017-01-01\", periods=era5_ds.sizes[\"band\"], freq=\"D\")\n",
    "era5_ds = era5_ds.assign_coords(band=dates).rename({\"band\": \"time\"})\n",
    "\n",
    "# Convert units: m/day → mm/day\n",
    "era5_ds = era5_ds * 1000  # 1 m/day = 1000 mm/day\n",
    "era5_ds.attrs[\"units\"] = \"mm/day\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproject, Clip, and Save (ERA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_era5_month(year, month, spatial_chunk=128):\n",
    "    \"\"\"Process ERA5 evaporation for one month.\"\"\"\n",
    "    try:\n",
    "        # Start Dask cluster\n",
    "        cluster = LocalCluster(n_workers=4, memory_limit='8GB')\n",
    "        client = Client(cluster)\n",
    "\n",
    "        # Slice to month and aggregate to monthly mean (instead of sum)\n",
    "        start_date = f\"{year}-{month:02d}-01\"\n",
    "        end_date = f\"{year}-{month:02d}-{pd.Timestamp(year, month, 1).daysinmonth}\"\n",
    "        # In process_era5_month():  \n",
    "        ds_month = era5_ds.sel(time=slice(start_date, end_date)).sum(dim=\"time\")  # Sum, not mean  \n",
    "        #ds_month = era5_ds.sel(time=slice(start_date, end_date)).mean(dim=\"time\")  # Monthly mean\n",
    "\n",
    "        # Reproject to NDVI grid (EPSG:26191, 10m)\n",
    "        ds_reproj = ds_month.rio.reproject(\n",
    "            \"EPSG:26191\",\n",
    "            resolution=10,\n",
    "            resampling=Resampling.bilinear,\n",
    "            nodata=np.nan\n",
    "        )\n",
    "\n",
    "        # Clip to Tadla boundary\n",
    "        tadla = gpd.read_file(tadla_common_path)\n",
    "        ds_clipped = ds_reproj.rio.clip(tadla.geometry, tadla.crs)\n",
    "\n",
    "        # Save\n",
    "        out_filename = weather_processed_dir / f\"ERA5_{year}_{month:02d}.tif\"\n",
    "        ds_clipped.rio.to_raster(out_filename)\n",
    "        print(f\"Saved {out_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {year}-{month:02d}: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all months\n",
    "for year in range(2017, 2024):\n",
    "    for month in range(1, 13):\n",
    "        process_era5_month(year, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Combining Monthly Files into Annual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "\n",
    "def reproject_annual(year, dataset=\"ERA5\"):\n",
    "    input_dir = Path(weather_processed_dir)\n",
    "    output_dir = Path(weather_processed_dir / f\"{dataset}_Annual/\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # List monthly files for the year\n",
    "    monthly_files = [input_dir / f\"{dataset}_{year}_{month:02d}.tif\" \n",
    "                     for month in range(1, 13)]\n",
    "    \n",
    "    # Reproject and stack bands\n",
    "    with rasterio.open(monthly_files[0]) as first:\n",
    "        meta = first.meta.copy()\n",
    "        meta.update(count=12)  # Explicitly set to 12 bands\n",
    "    \n",
    "    with rasterio.open(output_dir / f\"{dataset}_{year}_reproj.tif\", \"w\", **meta) as dst:\n",
    "        for band_idx, monthly_file in enumerate(monthly_files, start=1):\n",
    "            with rasterio.open(monthly_file) as src:\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, 1),\n",
    "                    destination=rasterio.band(dst, band_idx),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=meta[\"transform\"],\n",
    "                    dst_crs=meta[\"crs\"],\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "    print(f\"Processed {dataset}_{year}_reproj.tif with 12 bands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocess all years\n",
    "for year in range(2017, 2024):\n",
    "    reproject_annual(year, \"CHIRPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocess all years\n",
    "for year in range(2017, 2024):\n",
    "    reproject_annual(year, \"ERA5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Land Use/Crop Maps (Sentinel-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Authenticate & Initialize Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate (this will open a browser window for authentication if needed)\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initialize with your project settings (make sure you have set your GCP_PROJECT in your environment variables)\n",
    "ee.Initialize(project=project_id)\n",
    "\n",
    "print(\"Earth Engine has been initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadla = gpd.read_file(tadla_common_path)\n",
    "\n",
    "# Reproject to WGS84 (EPSG:4326) if needed\n",
    "if tadla.crs != \"EPSG:26191\":\n",
    "    tadla = tadla.to_crs(\"EPSG:26191\")\n",
    "\n",
    "# Convert to GEE geometry\n",
    "tadla_geom = ee.Geometry.Polygon(tadla.geometry[0].exterior.coords[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tadla boundary (WGS84)\n",
    "tadla_geom = ee.Geometry.Polygon(\n",
    "    [[-7.5, 32.0], [-5.5, 32.0], [-5.5, 32.8], [-7.5, 32.8]], \n",
    "    proj=\"EPSG:4326\", \n",
    "    geodesic=False\n",
    ")\n",
    "\n",
    "# Reproject to EPSG:26191 (Merchich)\n",
    "tadla_merc = tadla_geom.transform('EPSG:26191', 1)  # 1-meter error margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annual_composite(year):\n",
    "    start_date = f'{year}-04-01'\n",
    "    end_date = f'{year}-09-30'\n",
    "    \n",
    "    # Load Sentinel-2 collection\n",
    "    s2_collection = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n",
    "        .filterBounds(tadla_merc) \\\n",
    "        .filterDate(start_date, end_date) \\\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "    \n",
    "    # Harmonize bands: Select and rename critical bands (B4=Red, B8=NIR)\n",
    "    s2_harmonized = s2_collection.map(\n",
    "        lambda img: img.select(\n",
    "            ['B4', 'B8', 'SCL'],  # Keep only Red, NIR, and Scene Classification\n",
    "            ['red', 'nir', 'scl']  # Rename to avoid conflicts\n",
    "        ).cast({'red': 'float', 'nir': 'float'})  # Force consistent data types\n",
    "    )\n",
    "    \n",
    "    # Compute median composite\n",
    "    composite = s2_harmonized.median()\n",
    "    \n",
    "    # Calculate NDVI\n",
    "    ndvi = composite.expression(\n",
    "        '(nir - red) / (nir + red)', \n",
    "        {'nir': composite.select('nir'), 'red': composite.select('red')}\n",
    "    ).rename('NDVI')\n",
    "    \n",
    "    return ndvi.reproject(crs='EPSG:26191', scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ndvi(year):\n",
    "    ndvi = get_annual_composite(year)\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image=ndvi,\n",
    "        description=f'Sentinel2_Tadla_NDVI_{year}',\n",
    "        folder='Tadla_Project',\n",
    "        scale=10,\n",
    "        region=tadla_merc,\n",
    "        crs='EPSG:26191',\n",
    "        maxPixels=1e13,\n",
    "        fileFormat='GeoTIFF'\n",
    "    )\n",
    "    task.start()\n",
    "    print(f\"Exported {year}: Task ID {task.id}\")\n",
    "\n",
    "# Run for all years (2017–2023)\n",
    "for year in range(2017, 2024):\n",
    "    export_ndvi(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 – Data Harmonization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Resample Coarse Data (Soil/DEM) to 10m Resolution\n",
    "\n",
    "    Goal: Resample low-resolution datasets (e.g., SoilGrids at 250m) to match NDVI’s 10m grid.\n",
    "    Why: To align all datasets spatially for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from NDVI 2017\n",
    "with rasterio.open(ndvi_path) as ndvi_ref:\n",
    "    ndvi_transform = ndvi_ref.transform  # 10m resolution transform\n",
    "    ndvi_crs = ndvi_ref.crs             # CRS (EPSG:26191)\n",
    "    ndvi_width = ndvi_ref.width         # Number of columns\n",
    "    ndvi_height = ndvi_ref.height       # Number of rows\n",
    "\n",
    "print(f\"Reference CRS: {ndvi_crs}\")\n",
    "print(f\"Reference resolution: {ndvi_transform[0]}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.warp import reproject, Resampling\n",
    "import numpy as np\n",
    "\n",
    "# Paths (update with your actual paths)\n",
    "soil_clay_10m = soil_processed_dir / \"tadla_clay_10m.tif\"\n",
    "\n",
    "# Resample clay to 10m using NDVI’s grid\n",
    "with rasterio.open(soil_processed_path) as src:\n",
    "    # Initialize destination array with NDVI dimensions\n",
    "    dst_data = np.zeros((ndvi_height, ndvi_width), dtype=np.float32)\n",
    "    \n",
    "    reproject(\n",
    "        source=rasterio.band(src, 1),\n",
    "        destination=dst_data,\n",
    "        src_transform=src.transform,\n",
    "        dst_transform=ndvi_transform,\n",
    "        src_crs=src.crs,\n",
    "        dst_crs=ndvi_crs,\n",
    "        resampling=Resampling.bilinear  # Use \"nearest\" for categorical data\n",
    "    )\n",
    "    \n",
    "    # Save resampled clay\n",
    "    with rasterio.open(\n",
    "        soil_clay_10m,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=ndvi_height,\n",
    "        width=ndvi_width,\n",
    "        count=1,\n",
    "        dtype=np.float32,\n",
    "        crs=ndvi_crs,\n",
    "        transform=ndvi_transform,\n",
    "        nodata=src.nodata\n",
    "    ) as dst:\n",
    "        dst.write(dst_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(soil_clay_10m) as clay_resampled:\n",
    "    print(f\"Resampled clay resolution: {clay_resampled.res}\")  # Should be (10.0, 10.0)\n",
    "    print(f\"CRS: {clay_resampled.crs}\")  # Should match NDVI (EPSG:26191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_soil_layer(raw_path, processed_path, ndvi_transform, ndvi_crs, ndvi_height, ndvi_width):\n",
    "    with rasterio.open(raw_path) as src:\n",
    "        dst_data = np.zeros((ndvi_height, ndvi_width), dtype=np.float32)\n",
    "        reproject(\n",
    "            source=rasterio.band(src, 1),\n",
    "            destination=dst_data,\n",
    "            src_transform=src.transform,\n",
    "            dst_transform=ndvi_transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_crs=ndvi_crs,\n",
    "            resampling=Resampling.bilinear\n",
    "        )\n",
    "        with rasterio.open(\n",
    "            processed_path,\n",
    "            \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=ndvi_height,\n",
    "            width=ndvi_width,\n",
    "            count=1,\n",
    "            dtype=np.float32,\n",
    "            crs=ndvi_crs,\n",
    "            transform=ndvi_transform,\n",
    "            nodata=src.nodata\n",
    "        ) as dst:\n",
    "            dst.write(dst_data, 1)\n",
    "    print(f\"Resampled {raw_path.name} → {processed_path}\")\n",
    "\n",
    "# Example usage:\n",
    "soil_params = {\n",
    "    \"silt\": \"tadla_silt_processed.tif\",\n",
    "    \"sand\": \"tadla_sand_processed.tif\",\n",
    "    \"ocd\": \"tadla_ocd_processed.tif\",  # Organic carbon density\n",
    "    \"wv0010\": \"tadla_wv0010_processed.tif\"   # Water content at saturation\n",
    "}\n",
    "\n",
    "for param, filename in soil_params.items():\n",
    "    pre_processed_path = soil_processed_dir / filename\n",
    "    processed_path_10m = soil_processed_dir / f\"tadla_{param}_10m.tif\"\n",
    "    resample_soil_layer(pre_processed_path, processed_path_10m, ndvi_transform, ndvi_crs, ndvi_height, ndvi_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in [\"silt\", \"sand\", \"ocd\", \"wv0010\"]:\n",
    "    with rasterio.open(soil_processed_dir / f\"tadla_{param}_10m.tif\") as src:\n",
    "        print(f\"{param} resolution: {src.res}, CRS: {src.crs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample DEM (12.5m → 10m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (update with your actual paths)\n",
    "dem_raw = raw_data_dir / config[\"paths\"][\"dem_raw\"]\n",
    "dem_processed = processed_data_dir / config[\"paths\"][\"dem_processed\"]\n",
    "\n",
    "with rasterio.open(dem_raw) as src:\n",
    "    dst_data = np.zeros((ndvi_height, ndvi_width), dtype=np.float32)\n",
    "    reproject(\n",
    "        source=rasterio.band(src, 1),\n",
    "        destination=dst_data,\n",
    "        src_transform=src.transform,\n",
    "        dst_transform=ndvi_transform,\n",
    "        src_crs=src.crs,\n",
    "        dst_crs=ndvi_crs,\n",
    "        resampling=Resampling.bilinear  # Use cubic for elevation\n",
    "    )\n",
    "    with rasterio.open(\n",
    "        dem_processed,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=ndvi_height,\n",
    "        width=ndvi_width,\n",
    "        count=1,\n",
    "        dtype=np.float32,\n",
    "        crs=ndvi_crs,\n",
    "        transform=ndvi_transform,\n",
    "        nodata=src.nodata\n",
    "    ) as dst:\n",
    "        dst.write(dst_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(dem_processed) as src:\n",
    "    print(f\"DEM resolution: {src.res}, CRS: {src.crs}\")  # Should be (10.0, 10.0), EPSG:26191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample Slope & Aspect Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample slope (from 12.5m to 10m)  \n",
    "with rasterio.open(slope_path) as src:  \n",
    "    dst_data = np.zeros((ndvi_height, ndvi_width), dtype=np.float32)  \n",
    "    reproject(  \n",
    "        source=rasterio.band(src, 1),  \n",
    "        destination=dst_data,  \n",
    "        src_transform=src.transform,  \n",
    "        dst_transform=ndvi_transform, \n",
    "        src_crs=src.crs,\n",
    "        dst_crs=ndvi_crs, \n",
    "        resampling=Resampling.bilinear  \n",
    "    )  \n",
    "    # Save to slope_10m.tif  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Align All Rasters to NDVI Grid\n",
    "    \n",
    "    Goal: Ensure all datasets (soil, DEM, weather) are spatially aligned with the NDVI grid.\n",
    "    Why: Even minor misalignments will break ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Align Weather Data (CHIRPS Rainfall and ERA5 Evaporation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "\n",
    "# Load NDVI reference (e.g., NDVI_2017.tif)\n",
    "ndvi_ref = rxr.open_rasterio(ndvi_path)\n",
    "ndvi_transform = ndvi_ref.rio.transform()  # Get the exact transform\n",
    "ndvi_crs = ndvi_ref.rio.crs\n",
    "ndvi_shape = (ndvi_ref.rio.height, ndvi_ref.rio.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "\n",
    "def Reproject_CHIRPS(year):\n",
    "    # Open the annual file as a rioxarray DataArray\n",
    "    da = rioxarray.open_rasterio(weather_processed_dir /  f\"CHIRPS_Annual/CHIRPS_{year}.tif\")\n",
    "\n",
    "    # Reproject to your NDVI grid\n",
    "    da_reproj = da.rio.reproject(\n",
    "        dst_crs=ndvi_crs,\n",
    "        shape=ndvi_shape,\n",
    "        transform=ndvi_transform,\n",
    "        resampling=Resampling.bilinear\n",
    "    )\n",
    "\n",
    "    # Save the reprojected file\n",
    "    out_file = weather_processed_dir / f\"CHIRPS_Annual/CHIRPS_{year}_reproj.tif\"\n",
    "    da_reproj.rio.to_raster(out_file)\n",
    "    print(f\"Reprojected file saved to {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "for year in range(2017, 2024):\n",
    "    Reproject_CHIRPS(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "def Reproject_ERA5(year):\n",
    "    # Path to your annual ERA5 file (e.g., for 2017)\n",
    "    era5_file = weather_processed_dir / f\"ERA5_Annual/ERA5_{year}.tif\"\n",
    "\n",
    "    # Open the annual ERA5 file as a rioxarray DataArray\n",
    "    da = rioxarray.open_rasterio(era5_file)\n",
    "\n",
    "    # Reproject to the NDVI grid using your target CRS, shape, and transform\n",
    "    era5_reproj = da.rio.reproject(\n",
    "        dst_crs=ndvi_crs,\n",
    "        shape=ndvi_shape,\n",
    "        transform=ndvi_transform,\n",
    "        resampling=Resampling.bilinear\n",
    "    )\n",
    "\n",
    "    # Save the reprojected ERA5 file\n",
    "    out_file = weather_processed_dir / f\"ERA5_Annual/ERA5_{year}_reproj.tif\"\n",
    "    era5_reproj.rio.to_raster(out_file)\n",
    "    print(f\"Reprojected ERA5 file saved to {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2017, 2024):\n",
    "    Reproject_ERA5(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2017_reproj.tif\") as chirps, rasterio.open(weather_processed_dir / \"ERA5_Annual/ERA5_2017_reproj.tif\") as era5:\n",
    "    assert chirps.transform == era5.transform, \"Transform mismatch!\"\n",
    "    assert chirps.crs == era5.crs, \"CRS mismatch!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Validate Full Spatial Alignment\n",
    "\n",
    "    Goal: Ensure all rasters (soil, DEM, NDVI, weather) share the same origin, resolution, and CRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Check CRS \n",
    "\n",
    "    Goal: Confirm all datasets use EPSG:26191 (Merchich)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "# List of all processed files to validate\n",
    "layers = [\n",
    "    ndvi_path,                    # Reference NDVI (EPSG:26191)\n",
    "    land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2018.tif\",\n",
    "    land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2019.tif\",\n",
    "    land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2020.tif\",\n",
    "    land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2021.tif\",\n",
    "    land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2022.tif\",\n",
    "    land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2023.tif\",\n",
    "    soil_processed_dir / \"tadla_clay_10m.tif\",\n",
    "    dem_processed_path,\n",
    "    weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2017_reproj.tif\",\n",
    "    weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2018_reproj.tif\",\n",
    "    weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2019_reproj.tif\",\n",
    "    weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2020_reproj.tif\",\n",
    "    weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2021_reproj.tif\",\n",
    "    weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2022_reproj.tif\",\n",
    "    weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2023_reproj.tif\",\n",
    "    weather_processed_dir / \"ERA5_Annual/ERA5_2017_reproj.tif\",\n",
    "    weather_processed_dir / \"ERA5_Annual/ERA5_2018_reproj.tif\",\n",
    "    weather_processed_dir / \"ERA5_Annual/ERA5_2019_reproj.tif\",\n",
    "    weather_processed_dir / \"ERA5_Annual/ERA5_2020_reproj.tif\",\n",
    "    weather_processed_dir / \"ERA5_Annual/ERA5_2021_reproj.tif\",\n",
    "    weather_processed_dir / \"ERA5_Annual/ERA5_2022_reproj.tif\",\n",
    "    weather_processed_dir / \"ERA5_Annual/ERA5_2023_reproj.tif\"\n",
    "]\n",
    "\n",
    "for layer in layers:\n",
    "    with rasterio.open(layer) as src:\n",
    "        print(f\"{layer.name}: CRS = {src.crs}\")  # Should all print \"EPSG:26191\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Validate Transform & Resolution\n",
    "\n",
    "    Goal: Ensure all rasters have the same origin (transform[2], transform[5]) and 10m resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(ndvi_path) as ndvi_ref:\n",
    "    ref_transform = ndvi_ref.transform\n",
    "    print(f\"Reference transform: {ref_transform}\")\n",
    "\n",
    "for layer in layers:\n",
    "    with rasterio.open(layer) as src:\n",
    "        print(f\"{layer.name}:\")\n",
    "        print(f\"  Transform: {src.transform}\")\n",
    "        print(f\"  Resolution: {src.res}\")\n",
    "        assert src.transform == ref_transform, \"Transform mismatch!\"\n",
    "        assert src.res == (10.0, 10.0), \"Resolution mismatch!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Check Spatial Extents\n",
    "\n",
    "    Goal: Ensure all layers cover the exact same geographic area as NDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(ndvi_path) as ndvi_ref:\n",
    "    ref_bounds = ndvi_ref.bounds\n",
    "\n",
    "for layer in layers:\n",
    "    with rasterio.open(layer) as src:\n",
    "        layer_bounds = src.bounds\n",
    "        print(f\"{layer.name}:\")\n",
    "        print(f\"  Bounds: {layer_bounds}\")\n",
    "        assert np.allclose(layer_bounds, ref_bounds, atol=1e-2), \"Bounds mismatch!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4. Validate Pixel Grid Alignment\n",
    "\n",
    "    Goal: Ensure a point (e.g., (340000, 240000)) falls in the same pixel across all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y = 340000.0, 240000.0  # A point in Tadla\n",
    "\n",
    "def get_pixel_value(path, x, y):\n",
    "    with rasterio.open(path) as src:\n",
    "        row, col = src.index(x, y)\n",
    "        return src.read(1)[row, col]\n",
    "\n",
    "print(f\"NDVI value at ({sample_x}, {sample_y}): {get_pixel_value(ndvi_path, sample_x, sample_y)}\")\n",
    "print(f\"Clay value: {get_pixel_value(soil_clay_10m, sample_x, sample_y)}\")\n",
    "print(f\"DEM value: {get_pixel_value(dem_processed_path, sample_x, sample_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5. Handle NoData Consistency\n",
    "\n",
    "    Goal: Ensure all rasters use the same NoData value (e.g., -9999.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_NODATA = -9999.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    with rasterio.open(layer, \"r+\") as src:  # Open in read/write mode\n",
    "        # Update metadata\n",
    "        src.nodata = STANDARD_NODATA\n",
    "        # Replace existing NaN/None with STANDARD_NODATA (if needed)\n",
    "        data = src.read(1)\n",
    "        data[~np.isfinite(data)] = STANDARD_NODATA  # Handle NaNs\n",
    "        src.write(data, 1)\n",
    "    print(f\"Updated {layer.name}: NoData = {STANDARD_NODATA}\")\n",
    "\n",
    "# Process all years (2017–2023)\n",
    "for year in range(2017, 2024):\n",
    "    # Update CHIRPS\n",
    "    chirps_path = weather_processed_dir / f\"CHIRPS_Annual/CHIRPS_{year}_reproj.tif\"\n",
    "    if chirps_path.exists():\n",
    "        with rasterio.open(chirps_path, \"r+\") as src:\n",
    "            src.nodata = STANDARD_NODATA\n",
    "            data = src.read(1)\n",
    "            data[np.isnan(data)] = STANDARD_NODATA\n",
    "            src.write(data, 1)\n",
    "    \n",
    "    # Update ERA5\n",
    "    era5_path = weather_processed_dir / f\"CHIRPS_Annual/ERA5_{year}_reproj.tif\"\n",
    "    if era5_path.exists():\n",
    "        with rasterio.open(era5_path, \"r+\") as src:\n",
    "            src.nodata = STANDARD_NODATA\n",
    "            data = src.read(1)\n",
    "            data[np.isnan(data)] = STANDARD_NODATA\n",
    "            src.write(data, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update all NDVI layers (2017–2023)\n",
    "for year in range(2017, 2024):\n",
    "    ndvi_path = land_use_raw_dir / f\"Sentinel2_Tadla_NDVI_{year}.tif\"\n",
    "    if ndvi_path.exists():\n",
    "        with rasterio.open(ndvi_path, \"r+\") as src:\n",
    "            src.nodata = STANDARD_NODATA\n",
    "            data = src.read(1)\n",
    "            data[~np.isfinite(data)] = STANDARD_NODATA  # Handles NaN/Inf\n",
    "            src.write(data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6. Visual Inspection\n",
    "\n",
    "    Goal: Plot layers over each other to confirm alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Plot NDVI\n",
    "with rasterio.open(ndvi_path) as src:\n",
    "    ndvi = src.read(1)\n",
    "    ax[0].imshow(ndvi, cmap=\"YlGn\", vmin=0, vmax=1)\n",
    "    ax[0].set_title(\"NDVI\")\n",
    "\n",
    "# Plot clay %\n",
    "with rasterio.open(soil_clay_10m) as src:\n",
    "    clay = src.read(1)\n",
    "    ax[1].imshow(clay, cmap=\"Reds\", alpha=0.7)  # Overlay with transparency\n",
    "    ax[1].set_title(\"Clay % Overlay\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Pixel Replacement:\n",
    "    \n",
    "    Check if nan/None values in the original data were replaced with -9999.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2017_reproj.tif\") as src:  \n",
    "    data = src.read(1)  \n",
    "    print(f\"Unique values in CHIRPS: {np.unique(data)}\")  \n",
    "    # Should include -9999.0 but no NaN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(dem_processed_path) as src:  \n",
    "    plt.imshow(src.read(1), cmap=\"viridis\", vmin=0, vmax=1000)  \n",
    "    plt.colorbar(label=\"Elevation (m)\")  \n",
    "    plt.title(\"DEM with NoData=-9999\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(soil_processed_dir / \"tadla_clay_10m.tif\") as src:  \n",
    "    print(src.nodata)  # Should still be -9999.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nodata(path):\n",
    "    \"\"\"Check if NoData is standardized across all files matching a pattern.\"\"\"\n",
    "    for year in range(2017, 2024):\n",
    "        if path.exists():\n",
    "            with rasterio.open(path) as src:\n",
    "                print(f\"{path.name}: NoData = {src.nodata}\")\n",
    "                data = src.read(1)\n",
    "                assert np.nanmax(data) != np.nan, \"NaNs still present!\"\n",
    "                assert (data[data == STANDARD_NODATA].size > 0), \"NoData not replaced!\"\n",
    "\n",
    "# Example usage:\n",
    "check_nodata(weather_processed_dir / \"CHIRPS_Annual/\" / f\"CHIRPS_{year}_reproj.tif\")  # CHIRPS_2017_reproj.tif, etc.\n",
    "check_nodata(weather_processed_dir / \"ERA5_Annual/\" / f\"ERA5_{year}_reproj.tif\")    # ERA5_2017_reproj.tif, etc.\n",
    "check_nodata(land_use_raw_dir / f\"Sentinel_Tadla_NDVI_{year}.tif\")          # Sentinel2_Tadla_NDVI_2017.tif, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil layers\n",
    "soil_layers = [\"clay\", \"silt\", \"sand\", \"ocd\", \"wv0010\"]\n",
    "for param in soil_layers:\n",
    "    path = soil_processed_dir / f\"tadla_{param}_10m.tif\"\n",
    "    with rasterio.open(path, \"r+\") as src:\n",
    "        src.nodata = STANDARD_NODATA\n",
    "        data = src.read(1)\n",
    "        data[~np.isfinite(data)] = STANDARD_NODATA\n",
    "        src.write(data, 1)\n",
    "\n",
    "# DEM\n",
    "with rasterio.open(dem_processed_path, \"r+\") as src:\n",
    "    src.nodata = STANDARD_NODATA\n",
    "    data = src.read(1)\n",
    "    data[~np.isfinite(data)] = STANDARD_NODATA\n",
    "    src.write(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "static_layers = [\n",
    "    soil_processed_dir / \"tadla_clay_10m.tif\",\n",
    "    soil_processed_dir / \"tadla_silt_10m.tif\",\n",
    "    soil_processed_dir / \"tadla_sand_10m.tif\",\n",
    "    soil_processed_dir / \"tadla_ocd_10m.tif\",\n",
    "    soil_processed_dir / \"tadla_wv0010_10m.tif\",\n",
    "    dem_processed_path\n",
    "]\n",
    "\n",
    "for layer in static_layers:\n",
    "    with rasterio.open(layer) as src:\n",
    "        print(f\"{layer.name}: NoData = {src.nodata}\")  # Should all be -9999.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for layer in static_layers:\n",
    "    with rasterio.open(layer) as src:\n",
    "        data = src.read(1)\n",
    "        valid_data = data[data != src.nodata]  # Exclude NoData\n",
    "        print(f\"{layer.name}:\")\n",
    "        print(f\"  Min = {np.min(valid_data):.2f}, Max = {np.max(valid_data):.2f}\")\n",
    "\n",
    "# Expected ranges:\n",
    "# - Clay/Silt/Sand: 0–100% (sum ≈ 100% per pixel)\n",
    "# - OCD: 0–50 g/kg (organic carbon)\n",
    "# - WCS: 0–1 cm³/cm³ (water content)\n",
    "# - DEM: Elevation in meters (e.g., 0–1000m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(ndvi_path) as ndvi_ref:\n",
    "    ref_transform = ndvi_ref.transform\n",
    "    print(f\"NDVI transform: {ref_transform.to_gdal()}\")  # GDAL-style tuple\n",
    "\n",
    "with rasterio.open(soil_processed_dir / \"tadla_clay_10m.tif\") as src:\n",
    "    clay_transform = src.transform.to_gdal()\n",
    "    print(f\"Clay transform: {clay_transform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load NDVI’s grid definition (transform, CRS, resolution)\n",
    "with rasterio.open(ndvi_path) as ndvi_ref:\n",
    "    ref_profile = ndvi_ref.profile  # Includes transform, crs, etc.\n",
    "\n",
    "# List of soil layers to reproject\n",
    "soil_layers = [\n",
    "    \"tadla_clay_10m.tif\",\n",
    "    \"tadla_silt_10m.tif\",\n",
    "    \"tadla_sand_10m.tif\",\n",
    "    \"tadla_ocd_10m.tif\",\n",
    "    \"tadla_wv0010_10m.tif\"\n",
    "]\n",
    "\n",
    "for layer in soil_layers:\n",
    "    input_path = soil_processed_dir / layer\n",
    "    output_path = input_path.parent / f\"aligned_{layer}\"\n",
    "    \n",
    "    # Reproject to NDVI’s grid\n",
    "    with rasterio.open(input_path) as src:\n",
    "        data = src.read(1)\n",
    "        with rasterio.open(output_path, \"w\", **ref_profile) as dst:\n",
    "            reproject(\n",
    "                source=data,\n",
    "                destination=rasterio.band(dst, 1),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=ref_profile[\"transform\"],\n",
    "                dst_crs=ref_profile[\"crs\"],\n",
    "                resampling=Resampling.bilinear  # Use \"nearest\" for categorical data\n",
    "            )\n",
    "    print(f\"Aligned {layer} → {output_path}\")\n",
    "\n",
    "    # Replace old file with aligned version\n",
    "    output_path.replace(input_path)  # Overwrite original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(ndvi_path) as ndvi_ref:\n",
    "    ref_transform = ndvi_ref.transform\n",
    "\n",
    "for layer in soil_layers:\n",
    "    input_path = soil_processed_dir / layer\n",
    "    with rasterio.open(input_path) as src:\n",
    "        # Check transform alignment with tolerance\n",
    "        assert np.allclose(\n",
    "            list(src.transform), \n",
    "            list(ref_transform), \n",
    "            atol=1e-6  # Allow 0.001mm tolerance\n",
    "        ), f\"{layer} transform mismatch!\"\n",
    "        print(f\"{layer} is aligned ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(ndvi_path) as ndvi_ref:\n",
    "    ref_transform = ndvi_ref.transform\n",
    "    ref_crs = ndvi_ref.crs\n",
    "\n",
    "for layer in static_layers:\n",
    "    with rasterio.open(layer) as src:\n",
    "        print(f\"{layer.name}:\")\n",
    "        print(f\"  CRS: {src.crs}\")  # Should be EPSG:26191\n",
    "        print(f\"  Transform: {src.transform}\")  # Should match NDVI\n",
    "        print(f\"  Resolution: {src.res}\")  # Should be (10.0, 10.0)\n",
    "        assert src.transform == ref_transform, \"Transform mismatch!\"\n",
    "        assert src.crs == ref_crs, \"CRS mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Plot NDVI\n",
    "with rasterio.open(ndvi_path) as src:\n",
    "    ndvi = src.read(1)\n",
    "    ax[0].imshow(ndvi, cmap=\"YlGn\", vmin=0, vmax=1)\n",
    "    ax[0].set_title(\"NDVI\")\n",
    "\n",
    "# Plot clay %\n",
    "with rasterio.open(soil_processed_dir / \"tadla_clay_10m.tif\") as src:\n",
    "    clay = src.read(1)\n",
    "    ax[1].imshow(clay, cmap=\"Reds\", alpha=0.7)  # Overlay with transparency\n",
    "    ax[1].set_title(\"Clay % Overlay\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  Temporal Aggregation (All Years)\n",
    "\n",
    "    Goal: Convert daily CHIRPS rainfall and ERA5 evaporation into monthly aggregates for all years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Batch Process CHIRPS (Daily → Monthly Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_output_dir = Path(config[\"paths\"][\"harmonized_data\"]) / \"CHIRPS_monthly\"\n",
    "chirps_output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "chirps_dir = weather_processed_dir / \"CHIRPS_Annual/\" # Input: Daily CHIRPS files\n",
    "\n",
    "for year in range(2017, 2024):\n",
    "    # Load daily CHIRPS for the year (already reprojected to 10m grid)\n",
    "    chirps_daily = rxr.open_rasterio(chirps_dir / f\"CHIRPS_{year}_reproj.tif\", chunks={\"band\": -1, \"x\": 1000, \"y\": 1000})\n",
    "    \n",
    "    # Convert to xarray Dataset and rename band to \"time\"\n",
    "    dates = pd.date_range(start=f\"{year}-01-01\", periods=chirps_daily.sizes[\"band\"], freq=\"D\")\n",
    "    chirps_daily = chirps_daily.assign_coords(band=dates).rename({\"band\": \"time\"})\n",
    "    \n",
    "    # Resample to monthly sum (total rainfall per month)\n",
    "    chirps_monthly = chirps_daily.resample(time=\"1ME\").sum(skipna=False)  # skipna=False to retain NoData\n",
    "    \n",
    "    # Save as NetCDF (one file per year)\n",
    "    chirps_monthly.rio.to_raster(output_dir / f\"CHIRPS_monthly_{year}.nc\")\n",
    "    print(f\"Saved CHIRPS monthly for {year}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. Batch Process ERA5 Evaporation (Daily → Monthly Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_output_dir = Path(config[\"paths\"][\"harmonized_data\"]) / \"ERA5_monthly\"\n",
    "era5_output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = weather_processed_dir / \"ERA5_Annual/\"  # Input: Daily ERA5 files\n",
    "\n",
    "for year in range(2017, 2024):\n",
    "    # Load daily ERA5 evaporation (already reprojected)\n",
    "    era5_daily = rxr.open_rasterio(era5_dir / f\"ERA5_{year}_reproj.tif\", chunks={\"band\": -1, \"x\": 1000, \"y\": 1000})\n",
    "    \n",
    "    # Assign time coordinates\n",
    "    dates = pd.date_range(start=f\"{year}-01-01\", periods=era5_daily.sizes[\"band\"], freq=\"D\")\n",
    "    era5_daily = era5_daily.assign_coords(band=dates).rename({\"band\": \"time\"})\n",
    "    \n",
    "    # Resample to monthly total (sum of daily evaporation)\n",
    "    era5_monthly = era5_daily.resample(time=\"1ME\").sum(skipna=False)\n",
    "    \n",
    "    # Save\n",
    "    era5_monthly.rio.to_raster(output_dir / f\"ERA5_monthly_{year}.nc\")\n",
    "    print(f\"Saved ERA5 monthly for {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. Stack All Years into a Single Dataset\n",
    "\n",
    "    Goal: Combine all monthly data (2017–2023) into a single NetCDF file for ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Dask client for parallel processing (adjust based on your RAM)\n",
    "client = Client(n_workers=4, memory_limit='8GB')  # Example: 4 workers, 8GB each\n",
    "\n",
    "# Load CHIRPS and ERA5 with Dask chunks\n",
    "chirps_ds = xr.open_mfdataset(\n",
    "    [chirps_output_dir / f\"CHIRPS_monthly_{year}.nc\" for year in range(2017, 2024)],\n",
    "    chunks={\"time\": 12, \"x\": 1000, \"y\": 1000},  # 1 year per chunk\n",
    "    combine=\"nested\",\n",
    "    concat_dim=\"time\",\n",
    "    parallel=True\n",
    ").rename({\"Band1\": \"rainfall\"})\n",
    "\n",
    "era5_ds = xr.open_mfdataset(\n",
    "    [era5_output_dir / f\"ERA5_monthly_{year}.nc\" for year in range(2017, 2024)],\n",
    "    chunks={\"time\": 12, \"x\": 1000, \"y\": 1000},\n",
    "    combine=\"nested\",\n",
    "    concat_dim=\"time\",\n",
    "    parallel=True\n",
    ").rename({\"Band1\": \"evaporation\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all static soil layers (clay, silt, sand, OCD, WSC)\n",
    "soil_vars = [\"clay\", \"silt\", \"sand\", \"ocd\", \"wv0010\"]\n",
    "soil_ds = xr.merge([\n",
    "    rxr.open_rasterio(soil_processed_dir / f\"tadla_{var}_processed.tif\", chunks={\"x\": 1000, \"y\": 1000}).rename(var)\n",
    "    for var in soil_vars\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load static layers with Dask\n",
    "dem = rxr.open_rasterio(dem_processed_path, chunks={\"x\": 1000, \"y\": 1000}).rename(\"dem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Load NDVI (annual composites, reprojected to monthly)\n",
    "ndvi_files = [land_use_raw_dir / f\"Sentinel2_Tadla_NDVI_{year}.tif\" for year in range(2017, 2024)]\n",
    "ndvi_ds = xr.open_mfdataset(\n",
    "    ndvi_files,\n",
    "    combine=\"nested\",\n",
    "    concat_dim=\"time\"\n",
    ")\n",
    "\n",
    "# Rechunk the dataset after loading\n",
    "ndvi_ds = ndvi_ds.chunk({\"time\": 1, \"x\": 1000, \"y\": 1000})\n",
    "\n",
    "# Rename the variable if it exists\n",
    "if 'band' in ndvi_ds:\n",
    "    ndvi_ds = ndvi_ds.rename({\"band\": \"NDVI\"})\n",
    "else:\n",
    "    print(\"Variable 'band' not found in the dataset. Please check the dataset structure.\")\n",
    "\n",
    "# Continue with your processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variables in 1 NDVI file\n",
    "test_ds = xr.open_dataset(land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2017.tif\")\n",
    "print(test_ds.data_vars)  # Likely shows \"band\", not \"Band1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Assign time coordinates\n",
    "times = pd.date_range(start=\"2017-01-01\", periods=7, freq=\"YS\")  # Annual start dates\n",
    "ndvi_ds = ndvi_ds.assign_coords(time=times)\n",
    "\n",
    "# 2. Drop the redundant \"NDVI\" coordinate (created during concatenation)\n",
    "ndvi_ds = ndvi_ds.drop_vars(\"NDVI\")\n",
    "\n",
    "# 3. Rename \"band_data\" to \"NDVI\"\n",
    "ndvi_ds = ndvi_ds.rename({\"band_data\": \"NDVI\"})\n",
    "\n",
    "# 4. Squeeze out the singleton \"NDVI\" dimension (size=1)\n",
    "ndvi_ds = ndvi_ds.squeeze(\"NDVI\")  # Now dimensions are (time, y, x)\n",
    "\n",
    "# 5. Resample to monthly\n",
    "ndvi_monthly = ndvi_ds.NDVI.resample(time=\"1ME\").interpolate(\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all variables\n",
    "final_ds = xr.merge([soil_ds, dem, ndvi_ds, chirps_ds, era5_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ds[\"NDVI\"] = ndvi_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_ds.data_vars)\n",
    "# Expected output:\n",
    "# ['clay', 'silt', 'sand', 'ocd', 'wcs', 'dem', 'slope', 'NDVI', 'rainfall', 'evaporation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop CRS variable\n",
    "final_ds = final_ds.drop_vars(\"lambert_conformal_conic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types\n",
    "final_ds = final_ds.astype({\n",
    "    \"clay\": \"float32\",\n",
    "    \"dem\": \"float32\",\n",
    "    \"rainfall\": \"float32\",\n",
    "    \"evaporation\": \"float32\",\n",
    "    \"wv0010\": \"float32\", \n",
    "    \"sand\": \"float32\", \n",
    "    \"ocd\": \"float32\", \n",
    "    \"silt\": \"float32\", \n",
    "    \"NDVI\": \"float32\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_ds.data_vars)\n",
    "# Expected output:\n",
    "# ['clay', 'silt', 'sand', 'ocd', 'wcs', 'dem', 'slope', 'NDVI', 'rainfall', 'evaporation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Dask for 16GB RAM\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,          # 4 workers\n",
    "    threads_per_worker=3, # 4*3=12 threads\n",
    "    memory_limit=\"3GB\"    # 4*3GB=12GB total\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# Assuming soil_ds, dem, ndvi_ds, chirps_ds, era5_ds are already xarray objects\n",
    "\n",
    "# For static datasets (e.g., soil, dem), you might not need to chunk over time:\n",
    "soil_ds = soil_ds.chunk({'x': 256, 'y': 256})\n",
    "dem = dem.chunk({'x': 256, 'y': 256})\n",
    "\n",
    "# For time-varying datasets, you can chunk along the time dimension:\n",
    "ndvi_ds = ndvi_ds.chunk({'time': 12, 'x': 256, 'y': 256})\n",
    "chirps_ds = chirps_ds.chunk({'time': 12, 'x': 256, 'y': 256})\n",
    "era5_ds = era5_ds.chunk({'time': 12, 'x': 256, 'y': 256})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = pd.date_range(\"2017-01-01\", \"2023-12-31\", freq=\"YS\").year.tolist()\n",
    "\n",
    "def process_year(year):\n",
    "    try:\n",
    "        # Select data using lazy indexing\n",
    "        yearly_ds = xr.merge([\n",
    "            soil_ds.sel(time=str(year), method='nearest'),\n",
    "            dem,\n",
    "            ndvi_ds.sel(time=str(year)),\n",
    "            chirps_ds.sel(time=str(year)),\n",
    "            era5_ds.sel(time=str(year))\n",
    "        ], join='exact')\n",
    "        \n",
    "        # Optimized Zarr write\n",
    "        output_path = harmonized_dir / f\"tadla_ml_dataset_{year}.zarr\"\n",
    "        yearly_ds.to_zarr(output_path, consolidated=True, mode=\"w\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {year}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Submit all years in parallel\n",
    "futures = [client.submit(process_year, year) for year in years]\n",
    "\n",
    "# Monitor progress\n",
    "from dask.diagnostics import ProgressBar\n",
    "with ProgressBar():\n",
    "    results = [f.result() for f in futures]\n",
    "\n",
    "print(f\"Successfully processed {sum(results)}/7 years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope/Aspect Resolution Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def resample_to_10m(input_path, output_path, reference_dem_path):\n",
    "    \"\"\"Resample a raster to match the grid of a reference DEM.\"\"\"\n",
    "    # Load reference DEM metadata\n",
    "    with rasterio.open(reference_dem_path) as ref:\n",
    "        ref_profile = ref.profile\n",
    "        ref_transform = ref.transform\n",
    "        ref_shape = (ref.height, ref.width)\n",
    "        ref_crs = ref.crs\n",
    "\n",
    "    # Open input raster\n",
    "    with rasterio.open(input_path) as src:\n",
    "        # Initialize destination array\n",
    "        dst_data = np.zeros(ref_shape, dtype=src.dtypes[0])\n",
    "\n",
    "        # Reproject/resample\n",
    "        reproject(\n",
    "            source=rasterio.band(src, 1),\n",
    "            destination=dst_data,\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=ref_transform,\n",
    "            dst_crs=ref_crs,\n",
    "            resampling=Resampling.bilinear  # For continuous data\n",
    "        )\n",
    "\n",
    "        # Update metadata\n",
    "        ref_profile.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"dtype\": dst_data.dtype,\n",
    "            \"nodata\": src.nodata\n",
    "        })\n",
    "\n",
    "        # Write resampled raster\n",
    "        with rasterio.open(output_path, \"w\", **ref_profile) as dst:\n",
    "            dst.write(dst_data, 1)\n",
    "\n",
    "# Paths (update as needed)\n",
    "\n",
    "reference_dem = topography_processed_dir / \"tadla_dem_10m.tif\"\n",
    "\n",
    "layers_to_resample = [\n",
    "    topography_processed_dir / \"tadla_slope.tif\",\n",
    "    topography_processed_dir / \"tadla_aspect.tif\"\n",
    "]\n",
    "\n",
    "# Process each layer\n",
    "for input_path in layers_to_resample:\n",
    "    temp_output = input_path.with_stem(f\"temp_{input_path.stem}\")\n",
    "    \n",
    "    # Resample to 10m\n",
    "    resample_to_10m(input_path, temp_output, reference_dem)\n",
    "    \n",
    "    # Replace original file\n",
    "    temp_output.replace(input_path)\n",
    "    print(f\"Resampled {input_path.name} → 10m resolution\")\n",
    "\n",
    "print(\"✅ Slope/aspect resampling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_slope_aspect():\n",
    "    ref_path = dem_processed_path  # Reference DEM\n",
    "    \n",
    "    with rasterio.open(ref_path) as ref:\n",
    "        target_res = ref.res  # Should be (10.0, 10.0)\n",
    "        target_crs = ref.crs\n",
    "\n",
    "    issues = []\n",
    "    for layer in [\"tadla_slope.tif\", \"tadla_aspect.tif\"]:\n",
    "        path = topography_processed_dir / layer\n",
    "        with rasterio.open(path) as src:\n",
    "            if src.res != target_res:\n",
    "                issues.append(f\"{layer}: Resolution {src.res} ≠ {target_res}\")\n",
    "            if src.crs != target_crs:\n",
    "                issues.append(f\"{layer}: CRS {src.crs} ≠ {target_crs}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"❌ Slope/aspect issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\" - {issue}\")\n",
    "    else:\n",
    "        print(\"✅ Slope/aspect have correct resolution (10m) and CRS!\")\n",
    "\n",
    "validate_slope_aspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soil Texture Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in [\"clay\", \"silt\", \"sand\"]:\n",
    "    path = soil_raw_dir / f\"tadla_{param}.tif\"\n",
    "    with rasterio.open(path) as src:\n",
    "        data = src.read(1)\n",
    "        print(f\"{param} raw values:\")\n",
    "        print(f\"  Min: {data.min()}, Max: {data.max()}\")  # Should be 0–1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def process_soil_layer(input_path, output_path):\n",
    "    with rasterio.open(input_path) as src:\n",
    "        data = src.read(1).astype(np.float32)\n",
    "        \n",
    "        # Step 1: Scale SoilGrids values (÷10)\n",
    "        data = data / 10  # Converts 408 → 40.8%\n",
    "        \n",
    "        # Step 2: Clip to 0-100% (handle outliers)\n",
    "        data = np.clip(data, 0, 100)\n",
    "        \n",
    "        # Step 3: Save with 10m grid\n",
    "        profile = src.profile.copy()\n",
    "        profile.update(\n",
    "            dtype=rasterio.float32,\n",
    "            nodata=-9999,\n",
    "            driver=\"GTiff\"\n",
    "        )\n",
    "        with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "            dst.write(data, 1)\n",
    "\n",
    "# Example usage\n",
    "#for param in [\"clay\", \"silt\", \"sand\"]:\n",
    "for param in [\"clay\"]:\n",
    "    input_path = soil_processed_dir / f\"tadla_{param}_10m.tif\"\n",
    "    output_path = soil_processed_dir / f\"tadla_{param}_10m_p.tif\"\n",
    "    process_soil_layer(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in [\"clay\", \"silt\", \"sand\"]:\n",
    "    path = soil_processed_dir / f\"tadla_{param}_10m.tif\"\n",
    "    with rasterio.open(path) as src:\n",
    "        data = src.read(1)\n",
    "        print(f\"{param} processed:\")\n",
    "        print(f\"  Min: {data.min():.1f}%, Max: {data.max():.1f}%\")  # Should be 0–100%\n",
    "        print(src.res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_soil_texture():\n",
    "    layers = [\"clay\", \"silt\", \"sand\"]\n",
    "    soil_data = {}\n",
    "    nodata = -9999\n",
    "\n",
    "    # Load data\n",
    "    for param in layers:\n",
    "        path = soil_processed_dir / f\"tadla_{param}_10m.tif\"\n",
    "        with rasterio.open(path) as src:\n",
    "            soil_data[param] = src.read(1)\n",
    "    \n",
    "    # Mask valid pixels (all layers have data)\n",
    "    mask = (\n",
    "        (soil_data[\"clay\"] != nodata) & \n",
    "        (soil_data[\"silt\"] != nodata) & \n",
    "        (soil_data[\"sand\"] != nodata)\n",
    "    )\n",
    "    \n",
    "    # Calculate total texture\n",
    "    total = np.full_like(soil_data[\"clay\"], np.nan)\n",
    "    total[mask] = (\n",
    "        soil_data[\"clay\"][mask] + \n",
    "        soil_data[\"silt\"][mask] + \n",
    "        soil_data[\"sand\"][mask]\n",
    "    )\n",
    "    total[(total < 80) | (total > 100)] = -9999 \n",
    "\n",
    "    profile = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'nodata': nodata,\n",
    "        'width': soil_data[\"clay\"].shape[1],\n",
    "        'height': soil_data[\"clay\"].shape[0],\n",
    "        'count': 1,\n",
    "        'crs': src.crs,\n",
    "        'transform': src.transform\n",
    "    }\n",
    "    # Save masked total texture layer\n",
    "    with rasterio.open(soil_processed_dir / \"total_texture_masked.tif\", \"w\", **profile) as dst:\n",
    "        dst.write(total, 1)\n",
    "    # Validate only sums ≤100%\n",
    "    valid_pixels = total[~np.isnan(total)]\n",
    "    valid_pixels = valid_pixels[valid_pixels <= 100]  # Exclude >100%\n",
    "    invalid = np.where(valid_pixels < 80)[0]  # Allow 80-100%\n",
    "\n",
    "    \n",
    "    if len(invalid) > 0:\n",
    "        print(f\"⚠️ {len(invalid)} pixels ({len(invalid)/len(valid_pixels):.1%}) <80%\")\n",
    "    else:\n",
    "        print(\"✅ Valid soil texture sums (80-100%)\")\n",
    "\n",
    "validate_soil_texture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "\n",
    "def reproject_annual(year, dataset=\"CHIRPS\"):\n",
    "    input_dir = Path(weather_processed_dir / f\"{dataset}_monthly/\")\n",
    "    output_dir = Path(weather_processed_dir / f\"{dataset}_Annual/\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # List monthly files for the year\n",
    "    monthly_files = [input_dir / f\"{dataset}_{year}_{month:02d}.tif\" \n",
    "                     for month in range(1, 13)]\n",
    "    \n",
    "    # Reproject and stack bands\n",
    "    with rasterio.open(monthly_files[0]) as first:\n",
    "        meta = first.meta.copy()\n",
    "        meta.update(count=12)  # Explicitly set to 12 bands\n",
    "    \n",
    "    with rasterio.open(output_dir / f\"{dataset}_{year}_reproj.tif\", \"w\", **meta) as dst:\n",
    "        for band_idx, monthly_file in enumerate(monthly_files, start=1):\n",
    "            with rasterio.open(monthly_file) as src:\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, 1),\n",
    "                    destination=rasterio.band(dst, band_idx),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=meta[\"transform\"],\n",
    "                    dst_crs=meta[\"crs\"],\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "    print(f\"Processed {dataset}_{year}_reproj.tif with 12 bands\")\n",
    "\n",
    "# Reprocess all years\n",
    "for year in range(2017, 2024):\n",
    "    reproject_annual(year, \"CHIRPS\")\n",
    "    reproject_annual(year, \"ERA5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_temporal_bands():\n",
    "    datasets = [\"CHIRPS\", \"ERA5\"]\n",
    "    issues = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for year in range(2017, 2024):\n",
    "            path = Path(weather_processed_dir / f\"{dataset}_Annual/{dataset}_{year}_reproj.tif\")\n",
    "            if not path.exists():\n",
    "                issues.append(f\"Missing: {path}\")\n",
    "                continue\n",
    "            \n",
    "            with rasterio.open(path) as src:\n",
    "                if src.count != 12:\n",
    "                    issues.append(f\"{path.name}: {src.count} bands (expected 12)\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"❌ Temporal band issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\" - {issue}\")\n",
    "    else:\n",
    "        print(\"✅ All temporal files have 12 bands\")\n",
    "\n",
    "check_temporal_bands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "def check_time_coverage():\n",
    "    datasets = [\"CHIRPS\", \"ERA5\"]\n",
    "    issues = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for year in range(2017, 2024):\n",
    "            path = Path(harmonized_dir / f\"{dataset}_monthly/{dataset}_monthly_{year}.nc\")\n",
    "            if not path.exists():\n",
    "                issues.append(f\"Missing: {path}\")\n",
    "                continue\n",
    "            \n",
    "            ds = xr.open_dataset(path)\n",
    "            print(f\"Coordinates in {path.name}: {ds.coords}\")  # Print dataset coordinates\n",
    "            if 'time' not in ds.coords:\n",
    "                issues.append(f\"{path.name}: 'time' coordinate not found\")\n",
    "                continue\n",
    "            \n",
    "            if len(ds.time) != 12:\n",
    "                issues.append(f\"{path.name}: {len(ds.time)} months (expected 12)\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"❌ Temporal coverage issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\" - {issue}\")\n",
    "    else:\n",
    "        print(\"✅ All harmonized datasets have 12 months\")\n",
    "\n",
    "check_time_coverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "def print_layer_info(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        print(f\"Layer: {path}\")\n",
    "        print(f\"  CRS: {src.crs}\")\n",
    "        print(f\"  Resolution: {src.res}\")\n",
    "        print(f\"  Bounds: {src.bounds}\")\n",
    "        print(f\"  Shape: {src.shape}\\n\")\n",
    "\n",
    "# Reference layer (NDVI/DEM)\n",
    "print_layer_info(land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2023.tif\")\n",
    "\n",
    "# Problematic layer\n",
    "print_layer_info(weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2023_reproj.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.warp import reproject, Resampling\n",
    "import numpy as np\n",
    "\n",
    "# Reference: NDVI layer\n",
    "with rasterio.open(land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2023.tif\") as ref:\n",
    "    target_profile = ref.profile.copy()\n",
    "    target_profile.update(count=12)  # For 12-band files\n",
    "\n",
    "def reproject_annual(year, dataset=\"CHIRPS\"):\n",
    "    src_path = Path(weather_processed_dir / f\"{dataset}_Annual/{dataset}_{year}_reproj.tif\")\n",
    "    dst_path = src_path.with_name(f\"{dataset}_{year}_aligned.tif\")\n",
    "    \n",
    "    with rasterio.open(src_path) as src:\n",
    "        # Initialize destination array\n",
    "        dst_data = np.zeros((12, target_profile[\"height\"], target_profile[\"width\"]), dtype=src.dtypes[0])\n",
    "        \n",
    "        # Reproject each band\n",
    "        for band in range(1, 13):\n",
    "            reproject(\n",
    "                source=rasterio.band(src, band),\n",
    "                destination=dst_data[band-1],\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=target_profile[\"transform\"],\n",
    "                dst_crs=target_profile[\"crs\"],\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "    \n",
    "    # Save aligned file\n",
    "    with rasterio.open(dst_path, \"w\", **target_profile) as dst:\n",
    "        dst.write(dst_data)\n",
    "    \n",
    "    # Replace original file\n",
    "    dst_path.replace(src_path)\n",
    "\n",
    "# Reproject all years\n",
    "for year in range(2017, 2024):\n",
    "    reproject_annual(year, \"CHIRPS\")\n",
    "    reproject_annual(year, \"ERA5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spatial Alignment Check\n",
    "\n",
    "    Ensure all layers (soil, DEM, NDVI, weather) share the exact same grid (resolution, transform, CRS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "def validate_grid_alignment():\n",
    "    reference = land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2023.tif\"  # Example NDVI\n",
    "    \n",
    "    with rasterio.open(reference) as ref:\n",
    "        ref_transform = ref.transform\n",
    "        ref_shape = (ref.height, ref.width)\n",
    "        ref_crs = ref.crs\n",
    "    \n",
    "    layers = [\n",
    "        soil_processed_dir / \"tadla_clay_10m.tif\",\n",
    "        topography_processed_dir / \"tadla_dem_10m.tif\",\n",
    "        weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2023_reproj.tif\"\n",
    "    ]\n",
    "    \n",
    "    for layer in layers:\n",
    "        with rasterio.open(layer) as src:\n",
    "            if (src.transform != ref_transform) or (src.shape != ref_shape) or (src.crs != ref_crs):\n",
    "                print(f\"❌ Misaligned: {layer}\")\n",
    "                return\n",
    "    \n",
    "    print(\"✅ All layers aligned!\")\n",
    "\n",
    "validate_grid_alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Normalization Check\n",
    "\n",
    "    Verify that input features are normalized (e.g., 0–1 or z-scores) to avoid model bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normalization():\n",
    "    datasets = {\n",
    "        \"NDVI\": (land_use_raw_dir / \"Sentinel2_Tadla_NDVI_2023.tif\", (-1, 1)),\n",
    "        \"Clay\": (soil_processed_dir / \"tadla_clay_10m.tif\", (0, 100)),\n",
    "        \"CHIRPS\": (weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2023_reproj.tif\", (0, 500))  # mm/month\n",
    "    }\n",
    "    \n",
    "    for name, (path, expected_range) in datasets.items():\n",
    "        with rasterio.open(path) as src:\n",
    "            data = src.read(1)\n",
    "            valid_data = data[data != src.nodata]\n",
    "            min_val, max_val = valid_data.min(), valid_data.max()\n",
    "            \n",
    "            if (min_val < expected_range[0]) or (max_val > expected_range[1]):\n",
    "                print(f\"⚠️ {name}: Values ({min_val:.2f}-{max_val:.2f}) outside expected range {expected_range}\")\n",
    "            else:\n",
    "                print(f\"✅ {name}: Within {expected_range}\")\n",
    "\n",
    "check_normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stack Monthly NDVI into a 12-Band File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.merge import merge\n",
    "import numpy as np\n",
    "\n",
    "monthly_ndvi_files = [\n",
    "    Path(land_use_raw_dir / f\"NDVI_2017_{month}.tif\") \n",
    "    for month in range(1, 13)\n",
    "]\n",
    "\n",
    "# Read all monthly NDVI arrays\n",
    "bands = []\n",
    "for f in monthly_ndvi_files:\n",
    "    with rasterio.open(f) as src:\n",
    "        bands.append(src.read(1))\n",
    "\n",
    "# Create stacked array (bands, height, width)\n",
    "stacked = np.stack(bands, axis=0)\n",
    "\n",
    "# Copy metadata from first monthly file\n",
    "with rasterio.open(monthly_ndvi_files[0]) as src:\n",
    "    meta = src.meta.copy()\n",
    "\n",
    "# Update metadata for multi-band file\n",
    "meta.update(count=12)\n",
    "\n",
    "# Save stacked NDVI\n",
    "with rasterio.open(land_use_processed_dir / \"Sentinel2_Tadla_NDVI_2017.tif\", \"w\", **meta) as dst:\n",
    "    dst.write(stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temporal Consistency Check\n",
    "\n",
    "    Ensure time stamps align across datasets (e.g., NDVI and rainfall for the same month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_temporal_alignment():\n",
    "    # Open the raster files\n",
    "    ndvi_raster = rasterio.open(land_use_processed_dir / \"Sentinel2_Tadla_NDVI_2017.tif\")\n",
    "    rain_raster = rasterio.open(weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2017_reproj.tif\")\n",
    "    \n",
    "    # Print the number of bands in each raster\n",
    "    print(f\"NDVI raster bands: {ndvi_raster.count}\")\n",
    "    print(f\"Rainfall raster bands: {rain_raster.count}\")\n",
    "\n",
    "check_temporal_alignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_temporal_alignment():\n",
    "    # Check May 2017 (band index 5 for 0-based indexing)\n",
    "    may_2017_ndvi = rasterio.open(land_use_processed_dir / \"Sentinel2_Tadla_NDVI_2017.tif\").read(5)\n",
    "    may_2017_rain = rasterio.open(weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2017_reproj.tif\").read(5)\n",
    "    \n",
    "    if may_2017_ndvi.shape != may_2017_rain.shape:\n",
    "        print(\"❌ May 2017 NDVI and rainfall shapes mismatch\")\n",
    "    else:\n",
    "        print(\"✅ May 2017 temporal alignment OK\")\n",
    "\n",
    "check_temporal_alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NoData \n",
    "\n",
    "    Ensure NoData values (-9999) are consistently masked across all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nodata_presence():\n",
    "    layers = [\n",
    "        soil_processed_dir / \"tadla_clay_10m.tif\",\n",
    "        weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2023_reproj.tif\"\n",
    "    ]\n",
    "    \n",
    "    for path in layers:\n",
    "        with rasterio.open(path) as src:\n",
    "            data = src.read(1)\n",
    "            nodata_pct = (data == src.nodata).mean() * 100\n",
    "            print(f\"{Path(path).name}: {nodata_pct:.2f}% NoData\")\n",
    "\n",
    "check_nodata_presence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spatial-Temporal Leakage\n",
    "\n",
    "    Ensure your train/test split does not mix data from the same location or time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_train_test_split():\n",
    "    train_years = [2017, 2018, 2019, 2020, 2021]  # Example\n",
    "    test_years = [2022, 2023]\n",
    "    \n",
    "    # Ensure no overlap\n",
    "    assert not set(train_years).intersection(test_years), \"Leakage: Overlapping years!\"\n",
    "    \n",
    "    # Ensure spatial split (optional)\n",
    "    # Example: Train on northern half, test on southern half\n",
    "    with rasterio.open(land_use_processed_dir / \"Sentinel2_Tadla_NDVI_2023.tif\") as src:\n",
    "        height = src.height\n",
    "        train_mask = np.zeros((height, src.width), dtype=bool)\n",
    "        train_mask[:height//2, :] = True  # Northern half for training\n",
    "    \n",
    "    print(\"✅ Train/test split validated\")\n",
    "\n",
    "validate_train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Corruption Check\n",
    "\n",
    "    Ensure all files are readable and not corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_integrity():\n",
    "    all_files = [\n",
    "        Path(land_use_processed_dir / \"Sentinel2_Tadla_NDVI_2023.tif\"),\n",
    "        Path(weather_processed_dir / \"CHIRPS_Annual/CHIRPS_2023_reproj.tif\")\n",
    "    ]\n",
    "    \n",
    "    for path in all_files:\n",
    "        try:\n",
    "            with rasterio.open(path):\n",
    "                pass\n",
    "        except:\n",
    "            print(f\"❌ Corrupted file: {path}\")\n",
    "    print(\"✅ All files are readable\")\n",
    "\n",
    "check_file_integrity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hardware Readiness\n",
    "\n",
    "    Ensure your system can handle the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hardware():\n",
    "    import psutil\n",
    "    free_ram = psutil.virtual_memory().available / (1024 ** 3)  # GB\n",
    "    dataset_size = 12 * 9068 * 18904 * 4 / (1024 ** 3)  # Example: 12 bands, float32\n",
    "    print(f\"Free RAM: {free_ram:.1f} GB | Dataset size: {dataset_size:.1f} GB\")\n",
    "    \n",
    "    if free_ram < dataset_size * 2:\n",
    "        print(\"⚠️ Insufficient RAM – use batch loading or cloud compute\")\n",
    "\n",
    "check_hardware()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
